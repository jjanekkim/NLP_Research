{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Leakage\n",
    "\n",
    "## Summary\n",
    "\n",
    "This aspect of the research was particularly challenging due to the inability to inject a prompt into the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Prompt Leakage** in NLP, also known as prompt injection, is a vulnerability where an attacker manipulates the input to extract or alter the original prompt given to a language model. Here are some key use cases and implications:\n",
    "\n",
    "- `Data Security`: Prompt leakage can lead to unauthorized access to sensitive information embedded in the prompts, such as proprietary instructions or confidential data.\n",
    "- `Model Manipulation`: Attackers can exploit prompt leakage to manipulate the model’s behavior, causing it to generate unintended or harmful outputs.\n",
    "- `Intellectual Property Theft`: By leaking the original prompts, attackers can steal intellectual property, such as proprietary algorithms or unique prompt engineering techniques.\n",
    "- `Trust and Reliability`: Prompt leakage can undermine the trust and reliability of AI systems, especially in applications where accuracy and confidentiality are critical.\n",
    "- `Compliance`: Many industries have strict regulations regarding data privacy and security. Detecting and preventing prompt leakage helps customers comply with these regulations and avoid potential legal issues.\n",
    "\n",
    "Prompt leakage measures the extent to which sensitive or unintended information from the input prompt is exposed in the model’s output. This can include proprietary instructions, confidential data, or any other information that should remain hidden.\n",
    "\n",
    "- NLP Security: Prompt leakage is a critical aspect of NLP security, focusing on protecting the integrity and confidentiality of data processed by language models.\n",
    "- Prompt Engineering: Effective prompt engineering involves designing prompts that minimize the risk of leakage while still achieving the desired task performance.\n",
    "- Detection and Mitigation: Techniques such as canary tokens, input validation, and regular monitoring are employed to detect and mitigate prompt leakage. Libraries like Rebuff provide tools to enhance security against such vulnerabilities.\n",
    "\n",
    "**Prevent Prompt Leakage**\n",
    "\n",
    "1. `Input Validation and Sanitization`: Ensure that all inputs are thoroughly validated and sanitized to prevent malicious injections.\n",
    "2. `Use of Secure Prompts`: Design prompts that are less susceptible to manipulation. Avoid including sensitive information directly in the prompts.\n",
    "3. `Access Controls`: Implement strict access controls to limit who can interact with the model and modify prompts.\n",
    "4. `Regular Audits and Monitoring`: Conduct regular security audits and continuously monitor the system for any unusual activities or potential breaches.\n",
    "5. `Prompt Filtering`: Filter responses to detect and remove any unintended prompt leaks before they are processed or displayed.\n",
    "6. `Training and Awareness`: Educate developers and users about the risks of prompt leakage and best practices for secure prompt engineering.\n",
    "\n",
    "**Detecting Prompt Leakage**\n",
    "1. Canary Tokens\n",
    "2. Rebuff library : https://github.com/protectai/rebuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import rebuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt leakage not detected.\n",
      "Prompt leakage not detected.\n",
      "Prompt leakage detected!\n",
      "Prompt leakage detected!\n",
      "Prompt leakage detected!\n"
     ]
    }
   ],
   "source": [
    "def detect_prompt_leakage(output, canary_tokens):\n",
    "    for token in canary_tokens:\n",
    "        if token in output:\n",
    "            print(\"Prompt leakage detected!\")\n",
    "            return True\n",
    "    print(\"Prompt leakage not detected.\")\n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "canary_tokens = [\"Translate the following\", \"Summarize the following\", \"rm -rf /\"]\n",
    "outputs = [\n",
    "    \"J'aime la programmation.\",\n",
    "    \"AI is changing the world.\",\n",
    "    \"rm -rf /\",\n",
    "    \"Translate the following English sentence to French: 'I love programming.'\",\n",
    "    \"Summarize the following text: 'Artificial Intelligence is transforming the world.'\"\n",
    "]\n",
    "\n",
    "for output in outputs:\n",
    "    detect_prompt_leakage(output, canary_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
